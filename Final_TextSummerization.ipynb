{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_TextSummerization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vickeytomer007/TextSummerization/blob/master/Final_TextSummerization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeNXkas2-rCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "%matplotlib inline\n",
        "from gensim import corpora, models, similarities, matutils\n",
        "from sklearn import datasets\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCymvVz4PR_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "84b7c706-f963-4f5e-83d2-989e6e8d599c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RzohD-x-ttC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUtRy0WC-3yf",
        "colab_type": "code",
        "outputId": "6427bf64-dc87-42ef-b48b-1213be217c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "news_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_headline</th>\n",
              "      <th>news_article</th>\n",
              "      <th>news_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dua Lipa to perform at first-ever OnePlus Musi...</td>\n",
              "      <td>International pop star Dua Lipa will perform a...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OnePlus marks 1500 days of OxygenOS with plant...</td>\n",
              "      <td>OnePlus is celebrating 1500 days of OxygenOS a...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Disney CEO resigns from Apple board ahead of A...</td>\n",
              "      <td>Walt Disney CEO Bob Iger resigned from Apple's...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Google confirms its subscription service Play ...</td>\n",
              "      <td>Google took to Twitter to confirm that it's wo...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
              "      <td>Uber has awarded Indian ethical hacker Anand P...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       news_headline  ... news_category\n",
              "0  Dua Lipa to perform at first-ever OnePlus Musi...  ...    technology\n",
              "1  OnePlus marks 1500 days of OxygenOS with plant...  ...    technology\n",
              "2  Disney CEO resigns from Apple board ahead of A...  ...    technology\n",
              "3  Google confirms its subscription service Play ...  ...    technology\n",
              "4  Indian hacker finds Uber account takeover bug,...  ...    technology\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5flXs-Il-5GR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization.\n",
        "def token(text):\n",
        "    tok=[txt for txt in text.split()]\n",
        "    return tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCECiFRL-6pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df['headline_tok']=news_df['news_headline'].apply(lambda x:token(x))\n",
        "news_df['article_tok']=news_df['news_article'].apply(lambda x:token(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH2Fh4R1--Jh",
        "colab_type": "code",
        "outputId": "0d1ac6dd-7689-4324-e4ce-8c0809de6d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Removing Stopwords.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=stopwords.words('english')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKQbPHtq--ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Remove_Stopwords(text):\n",
        "    words=[txt for txt in text if txt not in stopwords]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHyFejRe-_3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df['headline_stop']=news_df['headline_tok'].apply(lambda x:Remove_Stopwords(x))\n",
        "news_df['article_stop']=news_df['article_tok'].apply(lambda x:Remove_Stopwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Ufk-Op_BAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_lower(text):\n",
        "    text=str(text)\n",
        "    words=\"\".join([txt for txt in text.lower()])\n",
        "    words=\"\".join(words)\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iGS6d49_CvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df['article_lower']=news_df['article_stop'].apply(lambda x:text_lower(x))\n",
        "news_df['headline_lower']=news_df['headline_stop'].apply(lambda x:text_lower(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKAsbENm_MgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joinText(text):\n",
        "    word=\" \".join(text)\n",
        "    return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJbCKzEl_Nto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#joinText(news_df['article_stop'][0])\n",
        "news_df['article_lower']=news_df['article_stop'].apply(lambda x:joinText(x))\n",
        "news_df['headline_lower']=news_df['headline_stop'].apply(lambda x:joinText(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJi6-ovM_O2L",
        "colab_type": "code",
        "outputId": "af8bc65f-c500-4b81-beb1-28fba71987de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbmkZULj_Q7r",
        "colab_type": "code",
        "outputId": "68dbf7d6-9486-4055-a307-d6dcc3971070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(\"Review #\",i+1)\n",
        "    print(news_df['headline_lower'][i])\n",
        "    print(news_df['article_lower'][i])\n",
        "    print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review # 1\n",
            "Dua Lipa perform first-ever OnePlus Music Festival India\n",
            "International pop star Dua Lipa perform first-ever OnePlus Music Festival set take place November 16 Mumbai. The festival take place DY Patil Stadium, singer Katy Perry would also performing. Tickets concert priced ₹2,000 onwards go sale 12 pm September 17.\n",
            "\n",
            "Review # 2\n",
            "OnePlus marks 1500 days OxygenOS plant tree campaign\n",
            "OnePlus celebrating 1500 days OxygenOS marking occasion collaborated WWF India 'Adopt Tree' campaign. OnePlus plant one tree every tweet #OxygenOS. \"It's amazing journey, building great products software together OnePlus community,\" said Szymon Kopec, Product lead OnePlus post Friday.\n",
            "\n",
            "Review # 3\n",
            "Disney CEO resigns Apple board ahead Apple TV+ launch\n",
            "Walt Disney CEO Bob Iger resigned Apple's board directors September 10, regulatory filings revealed. This comes ahead launch Apple's recently unveiled video streaming subscription service Apple TV+, stands direct rival entertainment conglomerate's service, Disney+. Iger became Apple director shortly Apple Co-founder Steve Jobs' death 2011.\n",
            "\n",
            "Review # 4\n",
            "Google confirms subscription service Play Pass launch 'soon'\n",
            "Google took Twitter confirm working service 'Play Pass', saying, \"It's almost time. Google Play Pass coming soon.\" The service earlier reported testing, would offer users \"hundreds premium apps games\" monthly fee. This comes days Apple announced subscription gaming service 'Apple Arcade'.\n",
            "\n",
            "Review # 5\n",
            "Indian hacker finds Uber account takeover bug, awarded ₹4.6 lakh\n",
            "Uber awarded Indian ethical hacker Anand Prakash ₹4.6 lakh finding bug allowed attackers take Uber user's account. Prakash requested public disclosure June bug report disclosed Uber September 9. Prakash, Forbes' 30 30 honoree, Founder cybersecurity company AppSecure.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZTb31Di_SGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DPofgws_TlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXAK14Qu_VDk",
        "colab_type": "code",
        "outputId": "3f91b3e4-70a0-4eba-c561-d17c1d189a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "  \n",
        "# Clean the summaries and texts\n",
        "clean_summaries = []\n",
        "for summary in news_df['headline_lower']:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in news_df['article_lower']:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkFCMHcC_WWM",
        "colab_type": "code",
        "outputId": "b05be387-6489-417b-bf42-15e90aeaeeb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
        "for i in range(5):\n",
        "    print(\"Clean Review #\",i+1)\n",
        "    print(news_df['headline_lower'][i])\n",
        "    print(news_df['article_lower'][i])\n",
        "    print()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean Review # 1\n",
            "Dua Lipa perform first-ever OnePlus Music Festival India\n",
            "International pop star Dua Lipa perform first-ever OnePlus Music Festival set take place November 16 Mumbai. The festival take place DY Patil Stadium, singer Katy Perry would also performing. Tickets concert priced ₹2,000 onwards go sale 12 pm September 17.\n",
            "\n",
            "Clean Review # 2\n",
            "OnePlus marks 1500 days OxygenOS plant tree campaign\n",
            "OnePlus celebrating 1500 days OxygenOS marking occasion collaborated WWF India 'Adopt Tree' campaign. OnePlus plant one tree every tweet #OxygenOS. \"It's amazing journey, building great products software together OnePlus community,\" said Szymon Kopec, Product lead OnePlus post Friday.\n",
            "\n",
            "Clean Review # 3\n",
            "Disney CEO resigns Apple board ahead Apple TV+ launch\n",
            "Walt Disney CEO Bob Iger resigned Apple's board directors September 10, regulatory filings revealed. This comes ahead launch Apple's recently unveiled video streaming subscription service Apple TV+, stands direct rival entertainment conglomerate's service, Disney+. Iger became Apple director shortly Apple Co-founder Steve Jobs' death 2011.\n",
            "\n",
            "Clean Review # 4\n",
            "Google confirms subscription service Play Pass launch 'soon'\n",
            "Google took Twitter confirm working service 'Play Pass', saying, \"It's almost time. Google Play Pass coming soon.\" The service earlier reported testing, would offer users \"hundreds premium apps games\" monthly fee. This comes days Apple announced subscription gaming service 'Apple Arcade'.\n",
            "\n",
            "Clean Review # 5\n",
            "Indian hacker finds Uber account takeover bug, awarded ₹4.6 lakh\n",
            "Uber awarded Indian ethical hacker Anand Prakash ₹4.6 lakh finding bug allowed attackers take Uber user's account. Prakash requested public disclosure June bug report disclosed Uber September 9. Prakash, Forbes' 30 30 honoree, Founder cybersecurity company AppSecure.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuatCibN_XiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ycf2GZv_Y7t",
        "colab_type": "code",
        "outputId": "ab640c08-fcf9-497d-e13d-dcea2f6880ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find the number of times each word was used and the size of the vocabulary\n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 3803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FlWLm-Q_aQy",
        "colab_type": "code",
        "outputId": "c932665a-d3e0-444e-955c-03f5311b04b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
        "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/Colab Notebooks/numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 484557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-acrO2L_bic",
        "colab_type": "code",
        "outputId": "9c92c6ef-f0ed-4806-85f5-a32fea5fabf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 1\n",
            "Percent of words that are missing from vocabulary: 0.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku5koNoB_cve",
        "colab_type": "code",
        "outputId": "6db9ce7d-8ca7-4e13-d1b8-eb79881cfcc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 3803\n",
            "Number of words we will use: 3506\n",
            "Percent of words we will use: 92.19000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYe7cvEY_eKQ",
        "colab_type": "code",
        "outputId": "4fe845a9-fa1e-425d-a03f-842dabca5ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhrbosOz_gl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHOYrEbC_ikj",
        "colab_type": "code",
        "outputId": "cc3af370-08ee-46c9-d47d-f011e9cd634b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 10592\n",
            "Total number of UNKs in headlines: 655\n",
            "Percent of words that are UNK: 6.18%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj2FX537_nJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHQ71H7H_okH",
        "colab_type": "code",
        "outputId": "ca5cac2b-3d5c-4c83-86d2-67077d3df97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "           counts\n",
            "count  224.000000\n",
            "mean     8.656250\n",
            "std      1.530748\n",
            "min      5.000000\n",
            "25%      8.000000\n",
            "50%      9.000000\n",
            "75%     10.000000\n",
            "max     13.000000\n",
            "\n",
            "Texts:\n",
            "           counts\n",
            "count  224.000000\n",
            "mean    39.629464\n",
            "std      4.660802\n",
            "min     27.000000\n",
            "25%     37.000000\n",
            "50%     39.000000\n",
            "75%     42.000000\n",
            "max     56.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JgosS3M_plp",
        "colab_type": "code",
        "outputId": "17546e94-c661-4bb2-9347-cf78f9c7aacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Inspect the length of texts\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46.0\n",
            "48.0\n",
            "53.77000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOPWApX_rIS",
        "colab_type": "code",
        "outputId": "36ca464e-da89-4a42-e8a0-feed70f64ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11.0\n",
            "11.0\n",
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYM9gs9g_sR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTcVUxQ1_tas",
        "colab_type": "code",
        "outputId": "25368c3c-d5af-4706-abfd-e3575409baba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# takes a long time  , this is normal\n",
        "\n",
        "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
        "# Limit the length of summaries and texts based on the min and max ranges.\n",
        "# Remove reviews that include too many UNKs\n",
        "\n",
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 84\n",
        "max_summary_length = 13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84\n",
            "84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorIcWuC_uj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_XpRcX_w--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE8tEdkc_yPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gChesz-_zeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZmcODC_0_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzOAdrnS_2Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEFchTMB_3RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8z7tz9T_4sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rbXlUUG_58T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doB2i7Wp_7JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 40\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t-J7Icu_8eB",
        "colab_type": "code",
        "outputId": "064f819b-6f3a-4277-c7cf-a528b0736adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESm8MPH_9ja",
        "colab_type": "code",
        "outputId": "5986b95e-a5f0-4b12-ee7a-e540f973f8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Subset the data for training\n",
        "start = 1\n",
        "end = start + 1000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 29\n",
            "The longest text length: 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gog5yR7__d5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89ead86f-28a5-48c7-862c-80c8f4ce21cf"
      },
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 1 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 3 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "checkpoint =\"/content/drive/My Drive/Colab Notebooks/best_model.ckpt\" #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/100 Batch    1/2 - Loss: 14.061, Seconds: 0.90\n",
            "Average loss for this update: -14.061\n",
            "New Record!\n",
            "Epoch   2/100 Batch    1/2 - Loss: 28.572, Seconds: 0.92\n",
            "Average loss for this update: -28.572\n",
            "No Improvement.\n",
            "Epoch   3/100 Batch    1/2 - Loss: 11.015, Seconds: 0.92\n",
            "Average loss for this update: -11.015\n",
            "New Record!\n",
            "Epoch   4/100 Batch    1/2 - Loss:  9.418, Seconds: 0.93\n",
            "Average loss for this update: -9.418\n",
            "New Record!\n",
            "Epoch   5/100 Batch    1/2 - Loss:  8.924, Seconds: 0.92\n",
            "Average loss for this update: -8.924\n",
            "New Record!\n",
            "Epoch   6/100 Batch    1/2 - Loss:  8.545, Seconds: 0.90\n",
            "Average loss for this update: -8.545\n",
            "New Record!\n",
            "Epoch   7/100 Batch    1/2 - Loss:  8.288, Seconds: 0.93\n",
            "Average loss for this update: -8.288\n",
            "New Record!\n",
            "Epoch   8/100 Batch    1/2 - Loss:  8.002, Seconds: 0.92\n",
            "Average loss for this update: -8.002\n",
            "New Record!\n",
            "Epoch   9/100 Batch    1/2 - Loss:  7.670, Seconds: 0.94\n",
            "Average loss for this update: -7.67\n",
            "New Record!\n",
            "Epoch  10/100 Batch    1/2 - Loss:  7.328, Seconds: 0.93\n",
            "Average loss for this update: -7.328\n",
            "New Record!\n",
            "Epoch  11/100 Batch    1/2 - Loss:  6.856, Seconds: 0.90\n",
            "Average loss for this update: -6.856\n",
            "New Record!\n",
            "Epoch  12/100 Batch    1/2 - Loss:  6.513, Seconds: 0.93\n",
            "Average loss for this update: -6.513\n",
            "New Record!\n",
            "Epoch  13/100 Batch    1/2 - Loss:  6.159, Seconds: 0.89\n",
            "Average loss for this update: -6.159\n",
            "New Record!\n",
            "Epoch  14/100 Batch    1/2 - Loss:  5.898, Seconds: 0.91\n",
            "Average loss for this update: -5.898\n",
            "New Record!\n",
            "Epoch  15/100 Batch    1/2 - Loss:  5.468, Seconds: 0.89\n",
            "Average loss for this update: -5.468\n",
            "New Record!\n",
            "Epoch  16/100 Batch    1/2 - Loss:  5.102, Seconds: 0.88\n",
            "Average loss for this update: -5.102\n",
            "New Record!\n",
            "Epoch  17/100 Batch    1/2 - Loss:  4.850, Seconds: 0.90\n",
            "Average loss for this update: -4.85\n",
            "New Record!\n",
            "Epoch  18/100 Batch    1/2 - Loss:  4.442, Seconds: 0.89\n",
            "Average loss for this update: -4.442\n",
            "New Record!\n",
            "Epoch  19/100 Batch    1/2 - Loss:  4.048, Seconds: 0.92\n",
            "Average loss for this update: -4.048\n",
            "New Record!\n",
            "Epoch  20/100 Batch    1/2 - Loss:  3.782, Seconds: 0.93\n",
            "Average loss for this update: -3.782\n",
            "New Record!\n",
            "Epoch  21/100 Batch    1/2 - Loss:  3.591, Seconds: 0.90\n",
            "Average loss for this update: -3.591\n",
            "New Record!\n",
            "Epoch  22/100 Batch    1/2 - Loss:  3.473, Seconds: 0.90\n",
            "Average loss for this update: -3.473\n",
            "New Record!\n",
            "Epoch  23/100 Batch    1/2 - Loss:  3.126, Seconds: 0.88\n",
            "Average loss for this update: -3.126\n",
            "New Record!\n",
            "Epoch  24/100 Batch    1/2 - Loss:  2.966, Seconds: 0.92\n",
            "Average loss for this update: -2.966\n",
            "New Record!\n",
            "Epoch  25/100 Batch    1/2 - Loss:  2.931, Seconds: 0.89\n",
            "Average loss for this update: -2.931\n",
            "New Record!\n",
            "Epoch  26/100 Batch    1/2 - Loss:  2.829, Seconds: 0.89\n",
            "Average loss for this update: -2.829\n",
            "New Record!\n",
            "Epoch  27/100 Batch    1/2 - Loss:  2.562, Seconds: 0.91\n",
            "Average loss for this update: -2.562\n",
            "New Record!\n",
            "Epoch  28/100 Batch    1/2 - Loss:  2.329, Seconds: 0.91\n",
            "Average loss for this update: -2.329\n",
            "New Record!\n",
            "Epoch  29/100 Batch    1/2 - Loss:  2.121, Seconds: 0.88\n",
            "Average loss for this update: -2.121\n",
            "New Record!\n",
            "Epoch  30/100 Batch    1/2 - Loss:  2.057, Seconds: 0.89\n",
            "Average loss for this update: -2.057\n",
            "New Record!\n",
            "Epoch  31/100 Batch    1/2 - Loss:  1.827, Seconds: 0.91\n",
            "Average loss for this update: -1.827\n",
            "New Record!\n",
            "Epoch  32/100 Batch    1/2 - Loss:  1.631, Seconds: 0.90\n",
            "Average loss for this update: -1.631\n",
            "New Record!\n",
            "Epoch  33/100 Batch    1/2 - Loss:  1.552, Seconds: 0.95\n",
            "Average loss for this update: -1.552\n",
            "New Record!\n",
            "Epoch  34/100 Batch    1/2 - Loss:  1.388, Seconds: 0.90\n",
            "Average loss for this update: -1.388\n",
            "New Record!\n",
            "Epoch  35/100 Batch    1/2 - Loss:  1.739, Seconds: 0.90\n",
            "Average loss for this update: -1.739\n",
            "No Improvement.\n",
            "Epoch  36/100 Batch    1/2 - Loss:  1.866, Seconds: 0.90\n",
            "Average loss for this update: -1.866\n",
            "No Improvement.\n",
            "Epoch  37/100 Batch    1/2 - Loss:  1.584, Seconds: 0.90\n",
            "Average loss for this update: -1.584\n",
            "No Improvement.\n",
            "Epoch  38/100 Batch    1/2 - Loss:  1.666, Seconds: 0.93\n",
            "Average loss for this update: -1.666\n",
            "No Improvement.\n",
            "Epoch  39/100 Batch    1/2 - Loss:  1.502, Seconds: 0.89\n",
            "Average loss for this update: -1.502\n",
            "No Improvement.\n",
            "Epoch  40/100 Batch    1/2 - Loss:  1.290, Seconds: 0.91\n",
            "Average loss for this update: -1.29\n",
            "New Record!\n",
            "Epoch  41/100 Batch    1/2 - Loss:  1.238, Seconds: 0.98\n",
            "Average loss for this update: -1.238\n",
            "New Record!\n",
            "Epoch  42/100 Batch    1/2 - Loss:  1.042, Seconds: 0.92\n",
            "Average loss for this update: -1.042\n",
            "New Record!\n",
            "Epoch  43/100 Batch    1/2 - Loss:  1.081, Seconds: 0.88\n",
            "Average loss for this update: -1.081\n",
            "No Improvement.\n",
            "Epoch  44/100 Batch    1/2 - Loss:  1.131, Seconds: 0.89\n",
            "Average loss for this update: -1.131\n",
            "No Improvement.\n",
            "Epoch  45/100 Batch    1/2 - Loss:  0.875, Seconds: 1.00\n",
            "Average loss for this update: -0.875\n",
            "New Record!\n",
            "Epoch  46/100 Batch    1/2 - Loss:  0.797, Seconds: 0.89\n",
            "Average loss for this update: -0.797\n",
            "New Record!\n",
            "Epoch  47/100 Batch    1/2 - Loss:  0.705, Seconds: 0.89\n",
            "Average loss for this update: -0.705\n",
            "New Record!\n",
            "Epoch  48/100 Batch    1/2 - Loss:  0.685, Seconds: 0.93\n",
            "Average loss for this update: -0.685\n",
            "New Record!\n",
            "Epoch  49/100 Batch    1/2 - Loss:  0.638, Seconds: 0.89\n",
            "Average loss for this update: -0.638\n",
            "New Record!\n",
            "Epoch  50/100 Batch    1/2 - Loss:  0.529, Seconds: 0.88\n",
            "Average loss for this update: -0.529\n",
            "New Record!\n",
            "Epoch  51/100 Batch    1/2 - Loss:  0.616, Seconds: 0.90\n",
            "Average loss for this update: -0.616\n",
            "No Improvement.\n",
            "Epoch  52/100 Batch    1/2 - Loss:  0.465, Seconds: 0.91\n",
            "Average loss for this update: -0.465\n",
            "New Record!\n",
            "Epoch  53/100 Batch    1/2 - Loss:  0.469, Seconds: 0.95\n",
            "Average loss for this update: -0.469\n",
            "No Improvement.\n",
            "Epoch  54/100 Batch    1/2 - Loss:  0.398, Seconds: 0.88\n",
            "Average loss for this update: -0.398\n",
            "New Record!\n",
            "Epoch  55/100 Batch    1/2 - Loss:  0.343, Seconds: 0.93\n",
            "Average loss for this update: -0.343\n",
            "New Record!\n",
            "Epoch  56/100 Batch    1/2 - Loss:  0.312, Seconds: 0.91\n",
            "Average loss for this update: -0.312\n",
            "New Record!\n",
            "Epoch  57/100 Batch    1/2 - Loss:  0.274, Seconds: 0.92\n",
            "Average loss for this update: -0.274\n",
            "New Record!\n",
            "Epoch  58/100 Batch    1/2 - Loss:  0.284, Seconds: 0.89\n",
            "Average loss for this update: -0.284\n",
            "No Improvement.\n",
            "Epoch  59/100 Batch    1/2 - Loss:  0.277, Seconds: 0.92\n",
            "Average loss for this update: -0.277\n",
            "No Improvement.\n",
            "Epoch  60/100 Batch    1/2 - Loss:  0.269, Seconds: 0.94\n",
            "Average loss for this update: -0.269\n",
            "New Record!\n",
            "Epoch  61/100 Batch    1/2 - Loss:  0.204, Seconds: 0.90\n",
            "Average loss for this update: -0.204\n",
            "New Record!\n",
            "Epoch  62/100 Batch    1/2 - Loss:  0.191, Seconds: 0.89\n",
            "Average loss for this update: -0.191\n",
            "New Record!\n",
            "Epoch  63/100 Batch    1/2 - Loss:  0.171, Seconds: 0.88\n",
            "Average loss for this update: -0.171\n",
            "New Record!\n",
            "Epoch  64/100 Batch    1/2 - Loss:  0.145, Seconds: 0.89\n",
            "Average loss for this update: -0.145\n",
            "New Record!\n",
            "Epoch  65/100 Batch    1/2 - Loss:  0.127, Seconds: 0.89\n",
            "Average loss for this update: -0.127\n",
            "New Record!\n",
            "Epoch  66/100 Batch    1/2 - Loss:  0.113, Seconds: 1.04\n",
            "Average loss for this update: -0.113\n",
            "New Record!\n",
            "Epoch  67/100 Batch    1/2 - Loss:  0.117, Seconds: 1.04\n",
            "Average loss for this update: -0.117\n",
            "No Improvement.\n",
            "Epoch  68/100 Batch    1/2 - Loss:  0.119, Seconds: 1.06\n",
            "Average loss for this update: -0.119\n",
            "No Improvement.\n",
            "Epoch  69/100 Batch    1/2 - Loss:  0.110, Seconds: 1.04\n",
            "Average loss for this update: -0.11\n",
            "New Record!\n",
            "Epoch  70/100 Batch    1/2 - Loss:  0.100, Seconds: 1.08\n",
            "Average loss for this update: -0.1\n",
            "New Record!\n",
            "Epoch  71/100 Batch    1/2 - Loss:  0.083, Seconds: 1.06\n",
            "Average loss for this update: -0.083\n",
            "New Record!\n",
            "Epoch  72/100 Batch    1/2 - Loss:  0.074, Seconds: 1.07\n",
            "Average loss for this update: -0.074\n",
            "New Record!\n",
            "Epoch  73/100 Batch    1/2 - Loss:  0.096, Seconds: 0.99\n",
            "Average loss for this update: -0.096\n",
            "No Improvement.\n",
            "Epoch  74/100 Batch    1/2 - Loss:  0.067, Seconds: 1.10\n",
            "Average loss for this update: -0.067\n",
            "New Record!\n",
            "Epoch  75/100 Batch    1/2 - Loss:  0.075, Seconds: 1.02\n",
            "Average loss for this update: -0.075\n",
            "No Improvement.\n",
            "Epoch  76/100 Batch    1/2 - Loss:  0.070, Seconds: 1.08\n",
            "Average loss for this update: -0.07\n",
            "No Improvement.\n",
            "Epoch  77/100 Batch    1/2 - Loss:  0.085, Seconds: 1.10\n",
            "Average loss for this update: -0.085\n",
            "No Improvement.\n",
            "Epoch  78/100 Batch    1/2 - Loss:  0.056, Seconds: 1.05\n",
            "Average loss for this update: -0.056\n",
            "New Record!\n",
            "Epoch  79/100 Batch    1/2 - Loss:  0.072, Seconds: 1.11\n",
            "Average loss for this update: -0.072\n",
            "No Improvement.\n",
            "Epoch  80/100 Batch    1/2 - Loss:  0.058, Seconds: 1.06\n",
            "Average loss for this update: -0.058\n",
            "No Improvement.\n",
            "Epoch  81/100 Batch    1/2 - Loss:  0.050, Seconds: 1.03\n",
            "Average loss for this update: -0.05\n",
            "New Record!\n",
            "Epoch  82/100 Batch    1/2 - Loss:  0.060, Seconds: 1.10\n",
            "Average loss for this update: -0.06\n",
            "No Improvement.\n",
            "Epoch  83/100 Batch    1/2 - Loss:  0.062, Seconds: 1.10\n",
            "Average loss for this update: -0.062\n",
            "No Improvement.\n",
            "Epoch  84/100 Batch    1/2 - Loss:  0.047, Seconds: 1.16\n",
            "Average loss for this update: -0.047\n",
            "New Record!\n",
            "Epoch  85/100 Batch    1/2 - Loss:  0.053, Seconds: 1.14\n",
            "Average loss for this update: -0.053\n",
            "No Improvement.\n",
            "Epoch  86/100 Batch    1/2 - Loss:  0.041, Seconds: 1.15\n",
            "Average loss for this update: -0.041\n",
            "New Record!\n",
            "Epoch  87/100 Batch    1/2 - Loss:  0.036, Seconds: 1.16\n",
            "Average loss for this update: -0.036\n",
            "New Record!\n",
            "Epoch  88/100 Batch    1/2 - Loss:  0.036, Seconds: 1.18\n",
            "Average loss for this update: -0.036\n",
            "No Improvement.\n",
            "Epoch  89/100 Batch    1/2 - Loss:  0.034, Seconds: 1.11\n",
            "Average loss for this update: -0.034\n",
            "New Record!\n",
            "Epoch  90/100 Batch    1/2 - Loss:  0.035, Seconds: 1.09\n",
            "Average loss for this update: -0.035\n",
            "No Improvement.\n",
            "Epoch  91/100 Batch    1/2 - Loss:  0.036, Seconds: 1.10\n",
            "Average loss for this update: -0.036\n",
            "No Improvement.\n",
            "Epoch  92/100 Batch    1/2 - Loss:  0.065, Seconds: 1.12\n",
            "Average loss for this update: -0.065\n",
            "No Improvement.\n",
            "Epoch  93/100 Batch    1/2 - Loss:  0.046, Seconds: 1.13\n",
            "Average loss for this update: -0.046\n",
            "No Improvement.\n",
            "Epoch  94/100 Batch    1/2 - Loss:  0.035, Seconds: 1.11\n",
            "Average loss for this update: -0.035\n",
            "No Improvement.\n",
            "Epoch  95/100 Batch    1/2 - Loss:  0.176, Seconds: 1.07\n",
            "Average loss for this update: -0.176\n",
            "No Improvement.\n",
            "Stopping Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ZbKvR1AAsa",
        "colab_type": "code",
        "outputId": "f3af20a7-6c53-49a3-8f55-25f17974bd8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpoint = \"/content/drive/My Drive/Colab Notebooks/best_model.ckpt\" \n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "    names = []\n",
        "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
        "names"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/best_model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['input',\n",
              " 'targets',\n",
              " 'learning_rate',\n",
              " 'keep_prob',\n",
              " 'summary_length',\n",
              " 'Const',\n",
              " 'max_dec_len',\n",
              " 'text_length',\n",
              " 'ReverseV2/axis',\n",
              " 'ReverseV2',\n",
              " 'embedding_lookup/params_0',\n",
              " 'embedding_lookup/axis',\n",
              " 'embedding_lookup',\n",
              " 'embedding_lookup/Identity',\n",
              " 'encoder_0/DropoutWrapperInit/Const',\n",
              " 'encoder_0/DropoutWrapperInit/Const_1',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Cast',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/read',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/read',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_0/ReverseSequence',\n",
              " 'encoder_1/DropoutWrapperInit/Const',\n",
              " 'encoder_1/DropoutWrapperInit/Const_1',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Cast',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/read',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/read',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_1/ReverseSequence',\n",
              " 'concat/axis',\n",
              " 'concat',\n",
              " 'StridedSlice/begin',\n",
              " 'StridedSlice/end',\n",
              " 'StridedSlice/strides',\n",
              " 'StridedSlice',\n",
              " 'Fill/dims',\n",
              " 'Fill/value',\n",
              " 'Fill',\n",
              " 'concat_1/axis',\n",
              " 'concat_1',\n",
              " 'embedding_lookup_1/params_0',\n",
              " 'embedding_lookup_1/axis',\n",
              " 'embedding_lookup_1',\n",
              " 'embedding_lookup_1/Identity',\n",
              " 'decoder_0/DropoutWrapperInit/Const',\n",
              " 'decoder_0/DropoutWrapperInit/Const_1',\n",
              " 'decoder_1/DropoutWrapperInit/Const',\n",
              " 'decoder_1/DropoutWrapperInit/Const_1',\n",
              " 'BahdanauAttention/Shape',\n",
              " 'BahdanauAttention/strided_slice/stack',\n",
              " 'BahdanauAttention/strided_slice/stack_1',\n",
              " 'BahdanauAttention/strided_slice/stack_2',\n",
              " 'BahdanauAttention/strided_slice',\n",
              " 'BahdanauAttention/SequenceMask/Const',\n",
              " 'BahdanauAttention/SequenceMask/Const_1',\n",
              " 'BahdanauAttention/SequenceMask/Range',\n",
              " 'BahdanauAttention/SequenceMask/ExpandDims/dim',\n",
              " 'BahdanauAttention/SequenceMask/ExpandDims',\n",
              " 'BahdanauAttention/SequenceMask/Cast',\n",
              " 'BahdanauAttention/SequenceMask/Less',\n",
              " 'BahdanauAttention/SequenceMask/Cast_1',\n",
              " 'BahdanauAttention/ones/shape_as_tensor',\n",
              " 'BahdanauAttention/ones/Const',\n",
              " 'BahdanauAttention/ones',\n",
              " 'BahdanauAttention/Shape_1',\n",
              " 'BahdanauAttention/concat/axis',\n",
              " 'BahdanauAttention/concat',\n",
              " 'BahdanauAttention/Reshape',\n",
              " 'BahdanauAttention/mul',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/shape',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/min',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/max',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/sub',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/mul',\n",
              " 'memory_layer/kernel/Initializer/random_uniform',\n",
              " 'memory_layer/kernel',\n",
              " 'memory_layer/kernel/Assign',\n",
              " 'memory_layer/kernel/read',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/axes',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/free',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Shape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Prod',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Prod_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/stack',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose_1/perm',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1/shape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/MatMul',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const_2',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat_1/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot',\n",
              " 'BahdanauAttention/Shape_2',\n",
              " 'BahdanauAttention/strided_slice_1/stack',\n",
              " 'BahdanauAttention/strided_slice_1/stack_1',\n",
              " 'BahdanauAttention/strided_slice_1/stack_2',\n",
              " 'BahdanauAttention/strided_slice_1',\n",
              " 'BahdanauAttention/Shape_3',\n",
              " 'BahdanauAttention/strided_slice_2/stack',\n",
              " 'BahdanauAttention/strided_slice_2/stack_1',\n",
              " 'BahdanauAttention/strided_slice_2/stack_2',\n",
              " 'BahdanauAttention/strided_slice_2',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_4',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_5',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_6',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_7',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/x',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Equal',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Const',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/All',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const_1',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const_2',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const_3',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_0',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_1',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_2',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_4',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert',\n",
              " 'AttentionWrapperZeroState/checked_cell_state',\n",
              " 'AttentionWrapperZeroState/checked_cell_state_1',\n",
              " 'AttentionWrapperZeroState/Const',\n",
              " 'AttentionWrapperZeroState/ExpandDims/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims',\n",
              " 'AttentionWrapperZeroState/concat/axis',\n",
              " 'AttentionWrapperZeroState/concat',\n",
              " 'AttentionWrapperZeroState/zeros/Const',\n",
              " 'AttentionWrapperZeroState/zeros',\n",
              " 'AttentionWrapperZeroState/Const_1',\n",
              " 'AttentionWrapperZeroState/ExpandDims_1/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_1',\n",
              " 'AttentionWrapperZeroState/zeros_1',\n",
              " 'AttentionWrapperZeroState/Const_2',\n",
              " 'AttentionWrapperZeroState/Const_3',\n",
              " 'AttentionWrapperZeroState/concat_1/axis',\n",
              " 'AttentionWrapperZeroState/concat_1',\n",
              " 'AttentionWrapperZeroState/zeros_2/Const',\n",
              " 'AttentionWrapperZeroState/zeros_2',\n",
              " 'AttentionWrapperZeroState/Const_4',\n",
              " 'AttentionWrapperZeroState/Const_5',\n",
              " 'AttentionWrapperZeroState/Const_6',\n",
              " 'AttentionWrapperZeroState/ExpandDims_2/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_2',\n",
              " 'AttentionWrapperZeroState/concat_2/axis',\n",
              " 'AttentionWrapperZeroState/concat_2',\n",
              " 'AttentionWrapperZeroState/zeros_3/Const',\n",
              " 'AttentionWrapperZeroState/zeros_3',\n",
              " 'AttentionWrapperZeroState/Const_7',\n",
              " 'AttentionWrapperZeroState/ExpandDims_3/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/Size/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/Size',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/is_rank/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/is_rank',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Equal/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Equal/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/equal_1/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/equal_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/exclude_partial_shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Assert/data_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Assert',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/Size/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/Size',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/is_rank/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/is_rank',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vYrMVkkACa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8qMLsPvAGfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "000a3f2d-59ec-46ee-c64f-cfd8647ee001"
      },
      "source": [
        "# Create your own review or use one from the dataset\n",
        "    #input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
        "                      #I think that I will try a green apple next time.\"\n",
        "    #text = text_to_seq(input_sentence)\n",
        "    random = np.random.randint(0,len(clean_texts))\n",
        "    input_sentence = clean_texts[random]\n",
        "    text = text_to_seq(clean_texts[random])\n",
        "\n",
        "#    checkpoint = \"best_model.ckpt\"\n",
        "checkpoint = \"/content/drive/My Drive/Colab Notebooks/best_model.ckpt\" \n",
        "    loaded_graph = tf.Graph()\n",
        "    with tf.Session(graph=loaded_graph) as sess:\n",
        "        # Load saved model\n",
        "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "        loader.restore(sess, checkpoint)\n",
        "\n",
        "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "        text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "        #Multiply by batch_size to match the model's input parameters\n",
        "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                          summary_length: [np.random.randint(5,8)], \n",
        "                                          text_length: [len(text)]*batch_size,\n",
        "                                          keep_prob: 1.0})[0] \n",
        "\n",
        "    # Remove the padding from the tweet\n",
        "    pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "    print('Original Text:', news_df['article_lower'][random])\n",
        "    print('Original summary:', news_df['headline_lower'][random])#clean_summaries[random]\n",
        "\n",
        "    print('\\nText')\n",
        "    print('  Word Ids:    {}'.format([i for i in text]))\n",
        "    print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "    print('\\nSummary')\n",
        "    print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "    print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/best_model.ckpt\n",
            "Original Text: Sanoj Raj Bihar, became first contestant win ₹1 crore Kaun Banega Crorepati season 11, said considers money father's. \"My father farmer. It's giving money him. It's money,\" stated. He added father complete studies due family conditions.\n",
            "Original summary: This father's money: Farmer's son ₹1 crore KBC\n",
            "\n",
            "Text\n",
            "  Word Ids:    [3502, 890, 2864, 525, 3, 2865, 246, 3502, 139, 2866, 2867, 2868, 745, 88, 1227, 165, 493, 876, 876, 877, 173, 493, 493, 1754, 1496, 876, 2869, 2870, 1064, 874, 2871]\n",
            "  Input Words: <UNK> raj bihar became first contestant win <UNK> crore kaun banega crorepati season 11 said considers money father father farmer giving money money stated added father complete studies due family conditions\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [387, 388, 389, 390, 357, 391, 392]\n",
            "  Response Words: melania criticised coat showing plane flying tower\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Ix3iWq884g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75a8e33a-c8b7-4913-e245-5acb4e37bdb3"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = [['So', 'easy', 'so', 'good','and','all','natural']]\n",
        "candidate = ['great', 'hot', 'snack']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(score)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F2zdD_iBq8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "643add6b-3db2-471a-d216-c2a6e56c1275"
      },
      "source": [
        "!pip install sumeval\n",
        "!python -m spacy download en"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/87/bfc0f9397b9421305863edfdd2dbea637e47204976cb5473535c856338f4/sumeval-0.2.2.tar.gz (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from sumeval) (0.9.6)\n",
            "Collecting sacrebleu>=1.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/e5/93d252182f7cbd4b59bb3ec5797e2ce33cfd6f5aadaf327db170cf4b7887/sacrebleu-1.4.2-py3-none-any.whl\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.2->sumeval) (3.6.6)\n",
            "Building wheels for collected packages: sumeval\n",
            "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sumeval: filename=sumeval-0.2.2-cp36-none-any.whl size=54535 sha256=41545e2add97d309f34c2fd2bfe3277e3bd60a3c99457b6453aac4eeecdd11ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/6f/57/19ceecab21445c88f3c565735fa1887b4cd18d340c972eb445\n",
            "Successfully built sumeval\n",
            "Installing collected packages: portalocker, sacrebleu, sumeval\n",
            "Successfully installed portalocker-1.5.2 sacrebleu-1.4.2 sumeval-0.2.2\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWfywYMfBtWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b876948a-4635-406b-aee9-6bb7ee885fa4"
      },
      "source": [
        "#https://github.com/chakki-works/sumeval\n",
        "#https://github.com/Tian312/awesome-text-summarization\n",
        "\n",
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "\n",
        "refrence_summary = \"So easy, so good and all natural\"\n",
        "model_summary = \"great hot snack\"\n",
        "\n",
        "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
        "\n",
        "rouge_1 = rouge.rouge_n(\n",
        "            summary=model_summary,\n",
        "            references=refrence_summary,\n",
        "            n=1)\n",
        "\n",
        "rouge_2 = rouge.rouge_n(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary],\n",
        "            n=2)\n",
        "\n",
        "rouge_l = rouge.rouge_l(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary])\n",
        "\n",
        "# You need spaCy to calculate ROUGE-BE\n",
        "\n",
        "rouge_be = rouge.rouge_be(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary])\n",
        "\n",
        "print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "    rouge_1, rouge_2, rouge_l, rouge_be\n",
        ").replace(\", \", \"\\n\"))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b.great=(amod)=>snack\n",
            "b.hot=(amod)=>snack\n",
            "ROUGE-1: 0\n",
            "ROUGE-2: 0\n",
            "ROUGE-L: 0\n",
            "ROUGE-BE: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku4dmAWqBwEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "60f77144-556d-4be1-a0bf-cc8bb292d31c"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "sentence = (\"\"\"My little Chihuahua and Jack Russell mix just loves these! She \n",
        "gobbles them up! They are a perfect\n",
        "size for her small mouth. According\n",
        "to her, they are very tasty.\"\"\",\"\")\n",
        "model = (\"My Dog Loves Them!\",\"\")\n",
        "summary = (\"Chihuahua mix perfect\",\"\")\n",
        "\n",
        "tfidf_matrix_sentence = tfidf_vectorizer.fit_transform(sentence) \n",
        "tfidf_matrix_model = tfidf_vectorizer.transform(model) \n",
        "tfidf_matrix_summary = tfidf_vectorizer.transform(summary) \n",
        "\n",
        "\n",
        "cosine = cosine_similarity(tfidf_matrix_sentence, tfidf_matrix_model)\n",
        "print(cosine)\n",
        "\n",
        "cosine = cosine_similarity(tfidf_matrix_sentence, tfidf_matrix_summary)\n",
        "print(cosine)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.29277002 0.        ]\n",
            " [0.         0.        ]]\n",
            "[[0.29277002 0.        ]\n",
            " [0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnOIqqVxBzTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}