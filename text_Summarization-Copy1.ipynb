{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_urls = ['https://inshorts.com/en/read/technology',\n",
    "             'https://inshorts.com/en/read/sports',\n",
    "             'https://inshorts.com/en/read/world',\n",
    "            'https://inshorts.com/en/read/politics',\n",
    "            'https://inshorts.com/en/read/business',\n",
    "             'https://inshorts.com/en/read/startup',\n",
    "            'https://inshorts.com/en/read/entertainment',\n",
    "            'https://inshorts.com/en/read/science',\n",
    "            'https://inshorts.com/en/read/automobile']\n",
    "\n",
    "def build_dataset(seed_urls):\n",
    "    news_data = []\n",
    "    for url in seed_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        \n",
    "        news_articles = [{'news_headline': headline.find('span', \n",
    "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
    "                          'news_article': article.find('div', \n",
    "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
    "                          'news_category': news_category}\n",
    "                         \n",
    "                            for headline, article in \n",
    "                             zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', \n",
    "                                               class_=[\"news-card-content news-right-box\"]))\n",
    "                        ]\n",
    "        news_data.extend(news_articles)\n",
    "        \n",
    "    df =  pd.DataFrame(news_data)\n",
    "    df = df[['news_headline', 'news_article', 'news_category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Katy Perry performs at first-ever OnePlus Musi...</td>\n",
       "      <td>The OnePlus Music Festival, held at Mumbai's D...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a weird case, obviously don't need to work...</td>\n",
       "      <td>Microsoft Co-founder Bill Gates, at a recent e...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thousands of stolen Disney+ accounts on sale f...</td>\n",
       "      <td>Thousands of Disney+ user accounts have report...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google's room-sized 331 LED bulb system create...</td>\n",
       "      <td>Google AI team has built a room-sized system c...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google removes '2020 Sikh Referendum' app afte...</td>\n",
       "      <td>Google has removed mobile application '2020 Si...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amazon CEO Bezos to visit India in January 202...</td>\n",
       "      <td>Amazon Founder and CEO Jeff Bezos will visit I...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Realme CEO tweets update about company's phone...</td>\n",
       "      <td>Realme India CEO Madhav Sheth recently tweeted...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apple announces special event for apps and gam...</td>\n",
       "      <td>Apple is hosting a special media event that is...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WhatsApp beta allows using one account on mult...</td>\n",
       "      <td>WhatsApp is testing a feature on iOS that lets...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No users were affected by latest Pegasus-like ...</td>\n",
       "      <td>WhatsApp has said that no users were affected ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  Katy Perry performs at first-ever OnePlus Musi...   \n",
       "1  I'm a weird case, obviously don't need to work...   \n",
       "2  Thousands of stolen Disney+ accounts on sale f...   \n",
       "3  Google's room-sized 331 LED bulb system create...   \n",
       "4  Google removes '2020 Sikh Referendum' app afte...   \n",
       "5  Amazon CEO Bezos to visit India in January 202...   \n",
       "6  Realme CEO tweets update about company's phone...   \n",
       "7  Apple announces special event for apps and gam...   \n",
       "8  WhatsApp beta allows using one account on mult...   \n",
       "9  No users were affected by latest Pegasus-like ...   \n",
       "\n",
       "                                        news_article news_category  \n",
       "0  The OnePlus Music Festival, held at Mumbai's D...    technology  \n",
       "1  Microsoft Co-founder Bill Gates, at a recent e...    technology  \n",
       "2  Thousands of Disney+ user accounts have report...    technology  \n",
       "3  Google AI team has built a room-sized system c...    technology  \n",
       "4  Google has removed mobile application '2020 Si...    technology  \n",
       "5  Amazon Founder and CEO Jeff Bezos will visit I...    technology  \n",
       "6  Realme India CEO Madhav Sheth recently tweeted...    technology  \n",
       "7  Apple is hosting a special media event that is...    technology  \n",
       "8  WhatsApp is testing a feature on iOS that lets...    technology  \n",
       "9  WhatsApp has said that no users were affected ...    technology  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = build_dataset(seed_urls)\n",
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           25\n",
       "world            25\n",
       "politics         25\n",
       "business         25\n",
       "startup          25\n",
       "technology       25\n",
       "science          25\n",
       "automobile       25\n",
       "entertainment    24\n",
       "Name: news_category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['news_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dua Lipa to perform at first-ever OnePlus Musi...</td>\n",
       "      <td>International pop star Dua Lipa will perform a...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnePlus marks 1500 days of OxygenOS with plant...</td>\n",
       "      <td>OnePlus is celebrating 1500 days of OxygenOS a...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney CEO resigns from Apple board ahead of A...</td>\n",
       "      <td>Walt Disney CEO Bob Iger resigned from Apple's...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google confirms its subscription service Play ...</td>\n",
       "      <td>Google took to Twitter to confirm that it's wo...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
       "      <td>Uber has awarded Indian ethical hacker Anand P...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  Dua Lipa to perform at first-ever OnePlus Musi...   \n",
       "1  OnePlus marks 1500 days of OxygenOS with plant...   \n",
       "2  Disney CEO resigns from Apple board ahead of A...   \n",
       "3  Google confirms its subscription service Play ...   \n",
       "4  Indian hacker finds Uber account takeover bug,...   \n",
       "\n",
       "                                        news_article news_category  \n",
       "0  International pop star Dua Lipa will perform a...    technology  \n",
       "1  OnePlus is celebrating 1500 days of OxygenOS a...    technology  \n",
       "2  Walt Disney CEO Bob Iger resigned from Apple's...    technology  \n",
       "3  Google took to Twitter to confirm that it's wo...    technology  \n",
       "4  Uber has awarded Indian ethical hacker Anand P...    technology  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization.\n",
    "def token(text):\n",
    "    tok=[txt for txt in text.split()]\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dua', 'Lipa', 'to', 'perform', 'at', 'first-ever', 'OnePlus']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXample.\n",
    "token('Dua Lipa to perform at first-ever OnePlus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['headline_tok']=news_df['news_headline'].apply(lambda x:token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['article_tok']=news_df['news_article'].apply(lambda x:token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>headline_tok</th>\n",
       "      <th>article_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dua Lipa to perform at first-ever OnePlus Musi...</td>\n",
       "      <td>International pop star Dua Lipa will perform a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Dua, Lipa, to, perform, at, first-ever, OnePl...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, will, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnePlus marks 1500 days of OxygenOS with plant...</td>\n",
       "      <td>OnePlus is celebrating 1500 days of OxygenOS a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[OnePlus, marks, 1500, days, of, OxygenOS, wit...</td>\n",
       "      <td>[OnePlus, is, celebrating, 1500, days, of, Oxy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney CEO resigns from Apple board ahead of A...</td>\n",
       "      <td>Walt Disney CEO Bob Iger resigned from Apple's...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Disney, CEO, resigns, from, Apple, board, ahe...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, from,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google confirms its subscription service Play ...</td>\n",
       "      <td>Google took to Twitter to confirm that it's wo...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Google, confirms, its, subscription, service,...</td>\n",
       "      <td>[Google, took, to, Twitter, to, confirm, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
       "      <td>Uber has awarded Indian ethical hacker Anand P...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, has, awarded, Indian, ethical, hacker, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  Dua Lipa to perform at first-ever OnePlus Musi...   \n",
       "1  OnePlus marks 1500 days of OxygenOS with plant...   \n",
       "2  Disney CEO resigns from Apple board ahead of A...   \n",
       "3  Google confirms its subscription service Play ...   \n",
       "4  Indian hacker finds Uber account takeover bug,...   \n",
       "\n",
       "                                        news_article news_category  \\\n",
       "0  International pop star Dua Lipa will perform a...    technology   \n",
       "1  OnePlus is celebrating 1500 days of OxygenOS a...    technology   \n",
       "2  Walt Disney CEO Bob Iger resigned from Apple's...    technology   \n",
       "3  Google took to Twitter to confirm that it's wo...    technology   \n",
       "4  Uber has awarded Indian ethical hacker Anand P...    technology   \n",
       "\n",
       "                                        headline_tok  \\\n",
       "0  [Dua, Lipa, to, perform, at, first-ever, OnePl...   \n",
       "1  [OnePlus, marks, 1500, days, of, OxygenOS, wit...   \n",
       "2  [Disney, CEO, resigns, from, Apple, board, ahe...   \n",
       "3  [Google, confirms, its, subscription, service,...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                         article_tok  \n",
       "0  [International, pop, star, Dua, Lipa, will, pe...  \n",
       "1  [OnePlus, is, celebrating, 1500, days, of, Oxy...  \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, from,...  \n",
       "3  [Google, took, to, Twitter, to, confirm, that,...  \n",
       "4  [Uber, has, awarded, Indian, ethical, hacker, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Word vector: it takes words and produces respective vector.\n",
    "#Count based: algo- GloVe(Global vector) creates a matrix of words * features.\n",
    "GloVe is much faster then the word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Stopwords(text):\n",
    "    words=[txt for txt in text if txt not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['headline_stop']=news_df['headline_tok'].apply(lambda x:Remove_Stopwords(x))\n",
    "news_df['article_stop']=news_df['article_tok'].apply(lambda x:Remove_Stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>headline_tok</th>\n",
       "      <th>article_tok</th>\n",
       "      <th>headline_stop</th>\n",
       "      <th>article_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dua Lipa to perform at first-ever OnePlus Musi...</td>\n",
       "      <td>International pop star Dua Lipa will perform a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Dua, Lipa, to, perform, at, first-ever, OnePl...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, will, pe...</td>\n",
       "      <td>[Dua, Lipa, perform, first-ever, OnePlus, Musi...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, perform,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnePlus marks 1500 days of OxygenOS with plant...</td>\n",
       "      <td>OnePlus is celebrating 1500 days of OxygenOS a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[OnePlus, marks, 1500, days, of, OxygenOS, wit...</td>\n",
       "      <td>[OnePlus, is, celebrating, 1500, days, of, Oxy...</td>\n",
       "      <td>[OnePlus, marks, 1500, days, OxygenOS, plant, ...</td>\n",
       "      <td>[OnePlus, celebrating, 1500, days, OxygenOS, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney CEO resigns from Apple board ahead of A...</td>\n",
       "      <td>Walt Disney CEO Bob Iger resigned from Apple's...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Disney, CEO, resigns, from, Apple, board, ahe...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, from,...</td>\n",
       "      <td>[Disney, CEO, resigns, Apple, board, ahead, Ap...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google confirms its subscription service Play ...</td>\n",
       "      <td>Google took to Twitter to confirm that it's wo...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Google, confirms, its, subscription, service,...</td>\n",
       "      <td>[Google, took, to, Twitter, to, confirm, that,...</td>\n",
       "      <td>[Google, confirms, subscription, service, Play...</td>\n",
       "      <td>[Google, took, Twitter, confirm, working, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
       "      <td>Uber has awarded Indian ethical hacker Anand P...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, has, awarded, Indian, ethical, hacker, ...</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, awarded, Indian, ethical, hacker, Anand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  Dua Lipa to perform at first-ever OnePlus Musi...   \n",
       "1  OnePlus marks 1500 days of OxygenOS with plant...   \n",
       "2  Disney CEO resigns from Apple board ahead of A...   \n",
       "3  Google confirms its subscription service Play ...   \n",
       "4  Indian hacker finds Uber account takeover bug,...   \n",
       "\n",
       "                                        news_article news_category  \\\n",
       "0  International pop star Dua Lipa will perform a...    technology   \n",
       "1  OnePlus is celebrating 1500 days of OxygenOS a...    technology   \n",
       "2  Walt Disney CEO Bob Iger resigned from Apple's...    technology   \n",
       "3  Google took to Twitter to confirm that it's wo...    technology   \n",
       "4  Uber has awarded Indian ethical hacker Anand P...    technology   \n",
       "\n",
       "                                        headline_tok  \\\n",
       "0  [Dua, Lipa, to, perform, at, first-ever, OnePl...   \n",
       "1  [OnePlus, marks, 1500, days, of, OxygenOS, wit...   \n",
       "2  [Disney, CEO, resigns, from, Apple, board, ahe...   \n",
       "3  [Google, confirms, its, subscription, service,...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                         article_tok  \\\n",
       "0  [International, pop, star, Dua, Lipa, will, pe...   \n",
       "1  [OnePlus, is, celebrating, 1500, days, of, Oxy...   \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, from,...   \n",
       "3  [Google, took, to, Twitter, to, confirm, that,...   \n",
       "4  [Uber, has, awarded, Indian, ethical, hacker, ...   \n",
       "\n",
       "                                       headline_stop  \\\n",
       "0  [Dua, Lipa, perform, first-ever, OnePlus, Musi...   \n",
       "1  [OnePlus, marks, 1500, days, OxygenOS, plant, ...   \n",
       "2  [Disney, CEO, resigns, Apple, board, ahead, Ap...   \n",
       "3  [Google, confirms, subscription, service, Play...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                        article_stop  \n",
       "0  [International, pop, star, Dua, Lipa, perform,...  \n",
       "1  [OnePlus, celebrating, 1500, days, OxygenOS, m...  \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, Apple...  \n",
       "3  [Google, took, Twitter, confirm, working, serv...  \n",
       "4  [Uber, awarded, Indian, ethical, hacker, Anand...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Dua', 'Lipa', 'perform', 'first-ever', 'OnePlus', 'Music', 'Festival', 'India']\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(news_df['headline_stop'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lower(text):\n",
    "    text=str(text)\n",
    "    words=\"\".join([txt for txt in text.lower()])\n",
    "    words=\"\".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['international', 'pop', 'star', 'dua', 'lipa', 'perform', 'first-ever', 'oneplus', 'music', 'festival', 'set', 'take', 'place', 'november', '16', 'mumbai.', 'the', 'festival', 'take', 'place', 'dy', 'patil', 'stadium,', 'singer', 'katy', 'perry', 'would', 'also', 'performing.', 'tickets', 'concert', 'priced', '₹2,000', 'onwards', 'go', 'sale', '12', 'pm', 'september', '17.']\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_lower('Dua Lipa to perform at')\n",
    "text_lower(news_df['article_stop'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>headline_tok</th>\n",
       "      <th>article_tok</th>\n",
       "      <th>headline_stop</th>\n",
       "      <th>article_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dua Lipa to perform at first-ever OnePlus Musi...</td>\n",
       "      <td>International pop star Dua Lipa will perform a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Dua, Lipa, to, perform, at, first-ever, OnePl...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, will, pe...</td>\n",
       "      <td>[Dua, Lipa, perform, first-ever, OnePlus, Musi...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, perform,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnePlus marks 1500 days of OxygenOS with plant...</td>\n",
       "      <td>OnePlus is celebrating 1500 days of OxygenOS a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[OnePlus, marks, 1500, days, of, OxygenOS, wit...</td>\n",
       "      <td>[OnePlus, is, celebrating, 1500, days, of, Oxy...</td>\n",
       "      <td>[OnePlus, marks, 1500, days, OxygenOS, plant, ...</td>\n",
       "      <td>[OnePlus, celebrating, 1500, days, OxygenOS, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney CEO resigns from Apple board ahead of A...</td>\n",
       "      <td>Walt Disney CEO Bob Iger resigned from Apple's...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Disney, CEO, resigns, from, Apple, board, ahe...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, from,...</td>\n",
       "      <td>[Disney, CEO, resigns, Apple, board, ahead, Ap...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google confirms its subscription service Play ...</td>\n",
       "      <td>Google took to Twitter to confirm that it's wo...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Google, confirms, its, subscription, service,...</td>\n",
       "      <td>[Google, took, to, Twitter, to, confirm, that,...</td>\n",
       "      <td>[Google, confirms, subscription, service, Play...</td>\n",
       "      <td>[Google, took, Twitter, confirm, working, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
       "      <td>Uber has awarded Indian ethical hacker Anand P...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, has, awarded, Indian, ethical, hacker, ...</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, awarded, Indian, ethical, hacker, Anand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  Dua Lipa to perform at first-ever OnePlus Musi...   \n",
       "1  OnePlus marks 1500 days of OxygenOS with plant...   \n",
       "2  Disney CEO resigns from Apple board ahead of A...   \n",
       "3  Google confirms its subscription service Play ...   \n",
       "4  Indian hacker finds Uber account takeover bug,...   \n",
       "\n",
       "                                        news_article news_category  \\\n",
       "0  International pop star Dua Lipa will perform a...    technology   \n",
       "1  OnePlus is celebrating 1500 days of OxygenOS a...    technology   \n",
       "2  Walt Disney CEO Bob Iger resigned from Apple's...    technology   \n",
       "3  Google took to Twitter to confirm that it's wo...    technology   \n",
       "4  Uber has awarded Indian ethical hacker Anand P...    technology   \n",
       "\n",
       "                                        headline_tok  \\\n",
       "0  [Dua, Lipa, to, perform, at, first-ever, OnePl...   \n",
       "1  [OnePlus, marks, 1500, days, of, OxygenOS, wit...   \n",
       "2  [Disney, CEO, resigns, from, Apple, board, ahe...   \n",
       "3  [Google, confirms, its, subscription, service,...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                         article_tok  \\\n",
       "0  [International, pop, star, Dua, Lipa, will, pe...   \n",
       "1  [OnePlus, is, celebrating, 1500, days, of, Oxy...   \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, from,...   \n",
       "3  [Google, took, to, Twitter, to, confirm, that,...   \n",
       "4  [Uber, has, awarded, Indian, ethical, hacker, ...   \n",
       "\n",
       "                                       headline_stop  \\\n",
       "0  [Dua, Lipa, perform, first-ever, OnePlus, Musi...   \n",
       "1  [OnePlus, marks, 1500, days, OxygenOS, plant, ...   \n",
       "2  [Disney, CEO, resigns, Apple, board, ahead, Ap...   \n",
       "3  [Google, confirms, subscription, service, Play...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                        article_stop  \n",
       "0  [International, pop, star, Dua, Lipa, perform,...  \n",
       "1  [OnePlus, celebrating, 1500, days, OxygenOS, m...  \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, Apple...  \n",
       "3  [Google, took, Twitter, confirm, working, serv...  \n",
       "4  [Uber, awarded, Indian, ethical, hacker, Anand...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['article_lower']=news_df['article_stop'].apply(lambda x:text_lower(x))\n",
    "news_df['headline_lower']=news_df['headline_stop'].apply(lambda x:text_lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinText(text):\n",
    "    word=\" \".join(text)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joinText(news_df['article_stop'][0])\n",
    "news_df['article_lower']=news_df['article_stop'].apply(lambda x:joinText(x))\n",
    "news_df['headline_lower']=news_df['headline_stop'].apply(lambda x:joinText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "      <th>headline_tok</th>\n",
       "      <th>article_tok</th>\n",
       "      <th>headline_stop</th>\n",
       "      <th>article_stop</th>\n",
       "      <th>article_lower</th>\n",
       "      <th>headline_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dua Lipa to perform at first-ever OnePlus Musi...</td>\n",
       "      <td>International pop star Dua Lipa will perform a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Dua, Lipa, to, perform, at, first-ever, OnePl...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, will, pe...</td>\n",
       "      <td>[Dua, Lipa, perform, first-ever, OnePlus, Musi...</td>\n",
       "      <td>[International, pop, star, Dua, Lipa, perform,...</td>\n",
       "      <td>International pop star Dua Lipa perform first-...</td>\n",
       "      <td>Dua Lipa perform first-ever OnePlus Music Fest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnePlus marks 1500 days of OxygenOS with plant...</td>\n",
       "      <td>OnePlus is celebrating 1500 days of OxygenOS a...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[OnePlus, marks, 1500, days, of, OxygenOS, wit...</td>\n",
       "      <td>[OnePlus, is, celebrating, 1500, days, of, Oxy...</td>\n",
       "      <td>[OnePlus, marks, 1500, days, OxygenOS, plant, ...</td>\n",
       "      <td>[OnePlus, celebrating, 1500, days, OxygenOS, m...</td>\n",
       "      <td>OnePlus celebrating 1500 days OxygenOS marking...</td>\n",
       "      <td>OnePlus marks 1500 days OxygenOS plant tree ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disney CEO resigns from Apple board ahead of A...</td>\n",
       "      <td>Walt Disney CEO Bob Iger resigned from Apple's...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Disney, CEO, resigns, from, Apple, board, ahe...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, from,...</td>\n",
       "      <td>[Disney, CEO, resigns, Apple, board, ahead, Ap...</td>\n",
       "      <td>[Walt, Disney, CEO, Bob, Iger, resigned, Apple...</td>\n",
       "      <td>Walt Disney CEO Bob Iger resigned Apple's boar...</td>\n",
       "      <td>Disney CEO resigns Apple board ahead Apple TV+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google confirms its subscription service Play ...</td>\n",
       "      <td>Google took to Twitter to confirm that it's wo...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Google, confirms, its, subscription, service,...</td>\n",
       "      <td>[Google, took, to, Twitter, to, confirm, that,...</td>\n",
       "      <td>[Google, confirms, subscription, service, Play...</td>\n",
       "      <td>[Google, took, Twitter, confirm, working, serv...</td>\n",
       "      <td>Google took Twitter confirm working service 'P...</td>\n",
       "      <td>Google confirms subscription service Play Pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
       "      <td>Uber has awarded Indian ethical hacker Anand P...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, has, awarded, Indian, ethical, hacker, ...</td>\n",
       "      <td>[Indian, hacker, finds, Uber, account, takeove...</td>\n",
       "      <td>[Uber, awarded, Indian, ethical, hacker, Anand...</td>\n",
       "      <td>Uber awarded Indian ethical hacker Anand Praka...</td>\n",
       "      <td>Indian hacker finds Uber account takeover bug,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  Dua Lipa to perform at first-ever OnePlus Musi...   \n",
       "1  OnePlus marks 1500 days of OxygenOS with plant...   \n",
       "2  Disney CEO resigns from Apple board ahead of A...   \n",
       "3  Google confirms its subscription service Play ...   \n",
       "4  Indian hacker finds Uber account takeover bug,...   \n",
       "\n",
       "                                        news_article news_category  \\\n",
       "0  International pop star Dua Lipa will perform a...    technology   \n",
       "1  OnePlus is celebrating 1500 days of OxygenOS a...    technology   \n",
       "2  Walt Disney CEO Bob Iger resigned from Apple's...    technology   \n",
       "3  Google took to Twitter to confirm that it's wo...    technology   \n",
       "4  Uber has awarded Indian ethical hacker Anand P...    technology   \n",
       "\n",
       "                                        headline_tok  \\\n",
       "0  [Dua, Lipa, to, perform, at, first-ever, OnePl...   \n",
       "1  [OnePlus, marks, 1500, days, of, OxygenOS, wit...   \n",
       "2  [Disney, CEO, resigns, from, Apple, board, ahe...   \n",
       "3  [Google, confirms, its, subscription, service,...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                         article_tok  \\\n",
       "0  [International, pop, star, Dua, Lipa, will, pe...   \n",
       "1  [OnePlus, is, celebrating, 1500, days, of, Oxy...   \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, from,...   \n",
       "3  [Google, took, to, Twitter, to, confirm, that,...   \n",
       "4  [Uber, has, awarded, Indian, ethical, hacker, ...   \n",
       "\n",
       "                                       headline_stop  \\\n",
       "0  [Dua, Lipa, perform, first-ever, OnePlus, Musi...   \n",
       "1  [OnePlus, marks, 1500, days, OxygenOS, plant, ...   \n",
       "2  [Disney, CEO, resigns, Apple, board, ahead, Ap...   \n",
       "3  [Google, confirms, subscription, service, Play...   \n",
       "4  [Indian, hacker, finds, Uber, account, takeove...   \n",
       "\n",
       "                                        article_stop  \\\n",
       "0  [International, pop, star, Dua, Lipa, perform,...   \n",
       "1  [OnePlus, celebrating, 1500, days, OxygenOS, m...   \n",
       "2  [Walt, Disney, CEO, Bob, Iger, resigned, Apple...   \n",
       "3  [Google, took, Twitter, confirm, working, serv...   \n",
       "4  [Uber, awarded, Indian, ethical, hacker, Anand...   \n",
       "\n",
       "                                       article_lower  \\\n",
       "0  International pop star Dua Lipa perform first-...   \n",
       "1  OnePlus celebrating 1500 days OxygenOS marking...   \n",
       "2  Walt Disney CEO Bob Iger resigned Apple's boar...   \n",
       "3  Google took Twitter confirm working service 'P...   \n",
       "4  Uber awarded Indian ethical hacker Anand Praka...   \n",
       "\n",
       "                                      headline_lower  \n",
       "0  Dua Lipa perform first-ever OnePlus Music Fest...  \n",
       "1  OnePlus marks 1500 days OxygenOS plant tree ca...  \n",
       "2  Disney CEO resigns Apple board ahead Apple TV+...  \n",
       "3  Google confirms subscription service Play Pass...  \n",
       "4  Indian hacker finds Uber account takeover bug,...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "Dua Lipa perform first-ever OnePlus Music Festival India\n",
      "International pop star Dua Lipa perform first-ever OnePlus Music Festival set take place November 16 Mumbai. The festival take place DY Patil Stadium, singer Katy Perry would also performing. Tickets concert priced ₹2,000 onwards go sale 12 pm September 17.\n",
      "\n",
      "Review # 2\n",
      "OnePlus marks 1500 days OxygenOS plant tree campaign\n",
      "OnePlus celebrating 1500 days OxygenOS marking occasion collaborated WWF India 'Adopt Tree' campaign. OnePlus plant one tree every tweet #OxygenOS. \"It's amazing journey, building great products software together OnePlus community,\" said Szymon Kopec, Product lead OnePlus post Friday.\n",
      "\n",
      "Review # 3\n",
      "Disney CEO resigns Apple board ahead Apple TV+ launch\n",
      "Walt Disney CEO Bob Iger resigned Apple's board directors September 10, regulatory filings revealed. This comes ahead launch Apple's recently unveiled video streaming subscription service Apple TV+, stands direct rival entertainment conglomerate's service, Disney+. Iger became Apple director shortly Apple Co-founder Steve Jobs' death 2011.\n",
      "\n",
      "Review # 4\n",
      "Google confirms subscription service Play Pass launch 'soon'\n",
      "Google took Twitter confirm working service 'Play Pass', saying, \"It's almost time. Google Play Pass coming soon.\" The service earlier reported testing, would offer users \"hundreds premium apps games\" monthly fee. This comes days Apple announced subscription gaming service 'Apple Arcade'.\n",
      "\n",
      "Review # 5\n",
      "Indian hacker finds Uber account takeover bug, awarded ₹4.6 lakh\n",
      "Uber awarded Indian ethical hacker Anand Prakash ₹4.6 lakh finding bug allowed attackers take Uber user's account. Prakash requested public disclosure June bug report disclosed Uber September 9. Prakash, Forbes' 30 30 honoree, Founder cybersecurity company AppSecure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print(news_df['headline_lower'][i])\n",
    "    print(news_df['article_lower'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\52070730\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "# Clean the summaries and texts\n",
    "clean_summaries = []\n",
    "for summary in news_df['headline_lower']:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in news_df['article_lower']:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "Dua Lipa perform first-ever OnePlus Music Festival India\n",
      "International pop star Dua Lipa perform first-ever OnePlus Music Festival set take place November 16 Mumbai. The festival take place DY Patil Stadium, singer Katy Perry would also performing. Tickets concert priced ₹2,000 onwards go sale 12 pm September 17.\n",
      "\n",
      "Clean Review # 2\n",
      "OnePlus marks 1500 days OxygenOS plant tree campaign\n",
      "OnePlus celebrating 1500 days OxygenOS marking occasion collaborated WWF India 'Adopt Tree' campaign. OnePlus plant one tree every tweet #OxygenOS. \"It's amazing journey, building great products software together OnePlus community,\" said Szymon Kopec, Product lead OnePlus post Friday.\n",
      "\n",
      "Clean Review # 3\n",
      "Disney CEO resigns Apple board ahead Apple TV+ launch\n",
      "Walt Disney CEO Bob Iger resigned Apple's board directors September 10, regulatory filings revealed. This comes ahead launch Apple's recently unveiled video streaming subscription service Apple TV+, stands direct rival entertainment conglomerate's service, Disney+. Iger became Apple director shortly Apple Co-founder Steve Jobs' death 2011.\n",
      "\n",
      "Clean Review # 4\n",
      "Google confirms subscription service Play Pass launch 'soon'\n",
      "Google took Twitter confirm working service 'Play Pass', saying, \"It's almost time. Google Play Pass coming soon.\" The service earlier reported testing, would offer users \"hundreds premium apps games\" monthly fee. This comes days Apple announced subscription gaming service 'Apple Arcade'.\n",
      "\n",
      "Clean Review # 5\n",
      "Indian hacker finds Uber account takeover bug, awarded ₹4.6 lakh\n",
      "Uber awarded Indian ethical hacker Anand Prakash ₹4.6 lakh finding bug allowed attackers take Uber user's account. Prakash requested public disclosure June bug report disclosed Uber September 9. Prakash, Forbes' 30 30 honoree, Founder cybersecurity company AppSecure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(news_df['headline_lower'][i])\n",
    "    print(news_df['article_lower'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 3803\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 484557\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 1\n",
      "Percent of words that are missing from vocabulary: 0.03%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 3803\n",
      "Number of words we will use: 3506\n",
      "Percent of words we will use: 92.19000000000001%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3506\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 10592\n",
      "Total number of UNKs in headlines: 655\n",
      "Percent of words that are UNK: 6.18%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "           counts\n",
      "count  224.000000\n",
      "mean     8.656250\n",
      "std      1.530748\n",
      "min      5.000000\n",
      "25%      8.000000\n",
      "50%      9.000000\n",
      "75%     10.000000\n",
      "max     13.000000\n",
      "\n",
      "Texts:\n",
      "           counts\n",
      "count  224.000000\n",
      "mean    39.629464\n",
      "std      4.660802\n",
      "min     27.000000\n",
      "25%     37.000000\n",
      "50%     39.000000\n",
      "75%     42.000000\n",
      "max     56.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0\n",
      "48.0\n",
      "53.77000000000001\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "11.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# takes a long time  , this is normal\n",
    "\n",
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
    "    #                                                                _zero_state_tensors(rnn_size, \n",
    "    #                                                                                    batch_size, \n",
    "    #                                                                                    tf.float32)) \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_decoder = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "        \n",
    "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_decoder = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "        \n",
    "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 40\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\52070730\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-009698795f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                                       \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                                       \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                                                       batch_size)\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Create tensors for the training logits and inference logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-605aa3e33839>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[1;34m(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0menc_embed_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_embed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdec_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_encoding_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-9390cb26606a>\u001b[0m in \u001b[0;36mencoding_layer\u001b[1;34m(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoder_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n\u001b[0;32m      8\u001b[0m             cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data for training\n",
    "start = 0\n",
    "end = start + 100\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 1 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "  \n",
    "tf.reset_default_graph()\n",
    "checkpoint =\"best_model.ckpt\" #300k sentence\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    #sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "                #saver = tf.train.Saver() \n",
    "                #saver.save(sess, checkpoint)\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "              \n",
    "                  \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"best_model.ckpt\" \n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    names = []\n",
    "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "#input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                  #I think that I will try a green apple next time.\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "#news_df['article_stop'].apply(lambda x:text_lower(x))\n",
    "#news_df['headline_lower']=news_df['headline_stop']\n",
    "\n",
    "print('Original Text:', news_df['article_lower'][random])\n",
    "print('Original summary:', news_df['headline_lower'][random])#clean_summaries[random]\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create your own review or use one from the dataset\n",
    "    #input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                      #I think that I will try a green apple next time.\"\n",
    "    #text = text_to_seq(input_sentence)\n",
    "    random = np.random.randint(0,len(clean_texts))\n",
    "    input_sentence = clean_texts[random]\n",
    "    text = text_to_seq(clean_texts[random])\n",
    "\n",
    "    checkpoint = \"best_model.ckpt\"\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "        loader.restore(sess, checkpoint)\n",
    "\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        #Multiply by batch_size to match the model's input parameters\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "\n",
    "    # Remove the padding from the tweet\n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "    print('Original Text:', news_df['article_lower'][random])\n",
    "    print('Original summary:', news_df['headline_lower'][random])#clean_summaries[random]\n",
    "\n",
    "    print('\\nText')\n",
    "    print('  Word Ids:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "    print('\\nSummary')\n",
    "    print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
